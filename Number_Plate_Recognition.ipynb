{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib64/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "/usr/lib64/python2.7/site-packages/cffi/model.py:531: UserWarning: 'point_conversion_form_t' has no values explicitly defined; guessing that it is equivalent to 'unsigned int'\n",
      "  % self._get_c_name())\n"
     ]
    }
   ],
   "source": [
    "#### Import the required libraries\n",
    "import collections\n",
    "import itertools\n",
    "import math\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import cv2\n",
    "import numpy\n",
    "import tensorflow as tf\n",
    "\n",
    "import common\n",
    "import model\n",
    "from moviepy.editor import VideoFileClip\n",
    "\n",
    "def make_scaled_ims(im, min_shape):\n",
    "    ratio = 1. / 2 ** 0.5\n",
    "    shape = (im.shape[0] / ratio, im.shape[1] / ratio)\n",
    "\n",
    "    while True:\n",
    "        shape = (int(shape[0] * ratio), int(shape[1] * ratio))\n",
    "        if shape[0] < min_shape[0] or shape[1] < min_shape[1]:\n",
    "            break\n",
    "        yield cv2.resize(im, (shape[1], shape[0]))\n",
    "\n",
    "\n",
    "def detect(im, param_vals):\n",
    "    \"\"\"\n",
    "    Detect number plates in an image.\n",
    "\n",
    "    :param im:\n",
    "        Image to detect number plates in.\n",
    "\n",
    "    :param param_vals:\n",
    "        Model parameters to use. These are the parameters output by the `train`\n",
    "        module.\n",
    "\n",
    "    :returns:\n",
    "        Iterable of `bbox_tl, bbox_br, letter_probs`, defining the bounding box\n",
    "        top-left and bottom-right corners respectively, and a 7,36 matrix\n",
    "        giving the probability distributions of each letter.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert the image to various scales.\n",
    "    scaled_ims = list(make_scaled_ims(im, model.WINDOW_SHAPE))\n",
    "\n",
    "    # Load the model which detects number plates over a sliding window.\n",
    "    x, y, params = model.get_detect_model()\n",
    "\n",
    "    # Execute the model at each scale.\n",
    "    with tf.Session(config=tf.ConfigProto()) as sess:\n",
    "        y_vals = []\n",
    "        for scaled_im in scaled_ims:\n",
    "            feed_dict = {x: numpy.stack([scaled_im])}\n",
    "            feed_dict.update(dict(zip(params, param_vals)))\n",
    "            y_vals.append(sess.run(y, feed_dict=feed_dict))\n",
    "    writer = tf.summary.FileWriter(\"logs/\", sess.graph)\n",
    "\n",
    "    # Interpret the results in terms of bounding boxes in the input image.\n",
    "    # Do this by identifying windows (at all scales) where the model predicts a\n",
    "    # number plate has a greater than 50% probability of appearing.\n",
    "    #\n",
    "    # To obtain pixel coordinates, the window coordinates are scaled according\n",
    "    # to the stride size, and pixel coordinates.\n",
    "    for i, (scaled_im, y_val) in enumerate(zip(scaled_ims, y_vals)):\n",
    "        for window_coords in numpy.argwhere(y_val[0, :, :, 0] >\n",
    "                                                       -math.log(1./0.99 - 1)):\n",
    "            letter_probs = (y_val[0,\n",
    "                                  window_coords[0],\n",
    "                                  window_coords[1], 1:].reshape(\n",
    "                                    7, len(common.CHARS)))\n",
    "            letter_probs = common.softmax(letter_probs)\n",
    "\n",
    "            img_scale = float(im.shape[0]) / scaled_im.shape[0]\n",
    "\n",
    "            bbox_tl = window_coords * (8, 4) * img_scale\n",
    "            bbox_size = numpy.array(model.WINDOW_SHAPE) * img_scale\n",
    "\n",
    "            present_prob = common.sigmoid(\n",
    "                               y_val[0, window_coords[0], window_coords[1], 0])\n",
    "\n",
    "            yield bbox_tl, bbox_tl + bbox_size, present_prob, letter_probs\n",
    "\n",
    "\n",
    "def _overlaps(match1, match2):\n",
    "    bbox_tl1, bbox_br1, _, _ = match1\n",
    "    bbox_tl2, bbox_br2, _, _ = match2\n",
    "    return (bbox_br1[0] > bbox_tl2[0] and\n",
    "            bbox_br2[0] > bbox_tl1[0] and\n",
    "            bbox_br1[1] > bbox_tl2[1] and\n",
    "            bbox_br2[1] > bbox_tl1[1])\n",
    "\n",
    "\n",
    "def _group_overlapping_rectangles(matches):\n",
    "    matches = list(matches)\n",
    "    num_groups = 0\n",
    "    match_to_group = {}\n",
    "    for idx1 in range(len(matches)):\n",
    "        for idx2 in range(idx1):\n",
    "            if _overlaps(matches[idx1], matches[idx2]):\n",
    "                match_to_group[idx1] = match_to_group[idx2]\n",
    "                break\n",
    "        else:\n",
    "            match_to_group[idx1] = num_groups\n",
    "            num_groups += 1\n",
    "\n",
    "    groups = collections.defaultdict(list)\n",
    "    for idx, group in match_to_group.items():\n",
    "        groups[group].append(matches[idx])\n",
    "\n",
    "    return groups\n",
    "\n",
    "\n",
    "def post_process(matches):\n",
    "    \"\"\"\n",
    "    Take an iterable of matches as returned by `detect` and merge duplicates.\n",
    "\n",
    "    Merging consists of two steps:\n",
    "      - Finding sets of overlapping rectangles.\n",
    "      - Finding the intersection of those sets, along with the code\n",
    "        corresponding with the rectangle with the highest presence parameter.\n",
    "\n",
    "    \"\"\"\n",
    "    groups = _group_overlapping_rectangles(matches)\n",
    "\n",
    "    for group_matches in groups.values():\n",
    "        mins = numpy.stack(numpy.array(m[0]) for m in group_matches)\n",
    "        maxs = numpy.stack(numpy.array(m[1]) for m in group_matches)\n",
    "        present_probs = numpy.array([m[2] for m in group_matches])\n",
    "        letter_probs = numpy.stack(m[3] for m in group_matches)\n",
    "\n",
    "        yield (numpy.max(mins, axis=0).flatten(),\n",
    "               numpy.min(maxs, axis=0).flatten(),\n",
    "               numpy.max(present_probs),\n",
    "               letter_probs[numpy.argmax(present_probs)])\n",
    "\n",
    "\n",
    "def letter_probs_to_code(letter_probs):\n",
    "    return \"\".join(common.CHARS[i] for i in numpy.argmax(letter_probs, axis=1))\n",
    "\n",
    "\n",
    "def process_image(image):\n",
    "  \n",
    "    #im = cv2.imread(image)\n",
    "    im_gray = cv2.cvtColor(image ,cv2.COLOR_BGR2GRAY) / 255.\n",
    "\n",
    "    f = numpy.load('weights.npz')\n",
    "    param_vals = [f[n] for n in sorted(f.files, key=lambda s: int(s[4:]))]\n",
    "    print (\"RUN\")\n",
    "\n",
    "    for pt1, pt2, present_prob, letter_probs in post_process(\n",
    "                                                  detect(im_gray, param_vals)):\n",
    "        pt1 = tuple(reversed(map(int, pt1)))\n",
    "        pt2 = tuple(reversed(map(int, pt2)))\n",
    "\n",
    "        code = letter_probs_to_code(letter_probs)\n",
    "\n",
    "        color = (0.0, 255.0, 0.0)\n",
    "        cv2.rectangle(image, pt1, pt2, color)\n",
    "\n",
    "        cv2.putText(image,\n",
    "                    code,\n",
    "                    pt1,\n",
    "                    cv2.FONT_HERSHEY_PLAIN,\n",
    "                    1.5,\n",
    "                    (0, 0, 0),\n",
    "                    thickness=5)\n",
    "\n",
    "        cv2.putText(image,\n",
    "                    code,\n",
    "                    pt1,\n",
    "                    cv2.FONT_HERSHEY_PLAIN,\n",
    "                    1.5,\n",
    "                    (255, 255, 255),\n",
    "                    thickness=2)\n",
    "\n",
    "    return(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply the function to each frame in video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count=list()\n",
    "write_output = 'out_uk_car.mp4'\n",
    "## To speed up the testing process you may want to try your pipeline on a shorter subclip of the video\n",
    "## To do so add .subclip(start_second,end_second) to the end of the line below\n",
    "## Where start_second and end_second are integer values representing the start and end of the subclip\n",
    "## You may also uncomment the following line for a subclip of the first n seconds\n",
    "#clip1 = VideoFileClip(\"project_video.mp4\").subclip(0,3)\n",
    "\n",
    "clip1 = VideoFileClip(\"in_uk_car.mp4\")\n",
    "white_clip = clip1.fl_image(process_image) \n",
    "white_clip.write_videofile(write_output, audio=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
